{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# for dirname, _, filenames in os.walk(\"./\"):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given 30 distinct datasets, each consisting of 26 color images of a lowercase alphabetical letter in cursive handwriting, train a neural network that can read in similar images of cursive handwriting and identify the letter that is written with a moderately high degree of accuracy (ideally ~80s/90s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded all files from the Google Drive directory, then did some manual inspection with each folder. Based on an initial surface-level analysis, I found that:\n",
    "- within the S10 directory were 4 directories titled S11, S12, S13, S14.\n",
    "    - 3 of these 4 nested directories contained the correct amount of lowercase letter images needed\n",
    "    - S13 contained duplicates for most of the 26 letters\n",
    "- the S11, S12, S13, and S14 directories not nested inside of the S10 directory were either all incomplete or empty\n",
    "- there were 2 folders that were duplicates of other existing folders (same images, same names, etc)\n",
    "- S33 was missing the image for a lowercase j\n",
    "- S34 was largely incomplete and contained 10 letters\n",
    "- S35 was completely empty\n",
    "- many directories either had inconsistent naming (.png for some files and no extension for others), unlabeled data (images were not labeled with the intended letter), and the file extension varied across each directory (.heic, .jpg, .png, etc)\n",
    "\n",
    "my final decisions for the data batch were to:\n",
    "- use .png as a universal file extension across all datasets\n",
    "- rename unlabeled images with their corresponding lowercase letter\n",
    "    - some datasets were messier than others; hence, some guesswork had to be done for certain directories\n",
    "- deleted duplicate images in the S13 directory nested within the S10 directory\n",
    "- use the S11, S12, S13, and S14 directories nested within the S10 directory, and deleted the empty/incomplete directories with the same names outside of S10\n",
    "- deleted S33, S34, and S35\n",
    "- renumbered directories so that they would not have gaps\n",
    "\n",
    "and in the end, our final data consists of:\n",
    "- 30 distinct directories labeled 1-30 in ./og_data\n",
    "- 26 color-images of lowercase letters in each directory, labeled (letter).png "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore the Data\n",
    "\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======DATASET 1=======\n",
      "\n",
      "length of dataset 1: 26\n",
      "\n",
      "=======DATASET 2=======\n",
      "\n",
      "length of dataset 2: 26\n",
      "\n",
      "=======DATASET 3=======\n",
      "\n",
      "length of dataset 3: 26\n",
      "\n",
      "=======DATASET 4=======\n",
      "\n",
      "length of dataset 4: 26\n",
      "\n",
      "=======DATASET 5=======\n",
      "\n",
      "length of dataset 5: 26\n",
      "\n",
      "=======DATASET 6=======\n",
      "\n",
      "length of dataset 6: 26\n",
      "\n",
      "=======DATASET 7=======\n",
      "\n",
      "length of dataset 7: 26\n",
      "\n",
      "=======DATASET 8=======\n",
      "\n",
      "length of dataset 8: 26\n",
      "\n",
      "=======DATASET 9=======\n",
      "\n",
      "length of dataset 9: 26\n",
      "\n",
      "=======DATASET 10=======\n",
      "\n",
      "length of dataset 10: 26\n",
      "\n",
      "=======DATASET 11=======\n",
      "\n",
      "length of dataset 11: 26\n",
      "\n",
      "=======DATASET 12=======\n",
      "\n",
      "length of dataset 12: 26\n",
      "\n",
      "=======DATASET 13=======\n",
      "\n",
      "length of dataset 13: 26\n",
      "\n",
      "=======DATASET 14=======\n",
      "\n",
      "length of dataset 14: 26\n",
      "\n",
      "=======DATASET 15=======\n",
      "\n",
      "length of dataset 15: 26\n",
      "\n",
      "=======DATASET 16=======\n",
      "\n",
      "length of dataset 16: 26\n",
      "\n",
      "=======DATASET 17=======\n",
      "\n",
      "length of dataset 17: 25\n",
      "['a.png', 'b.png', 'c.png', 'd.png', 'e.png', 'f.png', 'g.png', 'h.png', 'i.png', 'j.png', 'k.png', 'l.png', 'm.png', 'o.png', 'p.png', 'q.png', 'r.png', 's.png', 't.png', 'u.png', 'v.png', 'w.png', 'x.png', 'y.png', 'z.png']\n",
      "\n",
      "=======DATASET 18=======\n",
      "\n",
      "length of dataset 18: 26\n",
      "\n",
      "=======DATASET 19=======\n",
      "\n",
      "length of dataset 19: 26\n",
      "\n",
      "=======DATASET 20=======\n",
      "\n",
      "length of dataset 20: 26\n",
      "\n",
      "=======DATASET 21=======\n",
      "\n",
      "length of dataset 21: 26\n",
      "\n",
      "=======DATASET 22=======\n",
      "\n",
      "length of dataset 22: 26\n",
      "\n",
      "=======DATASET 23=======\n",
      "\n",
      "length of dataset 23: 26\n",
      "\n",
      "=======DATASET 24=======\n",
      "\n",
      "length of dataset 24: 26\n",
      "\n",
      "=======DATASET 25=======\n",
      "\n",
      "length of dataset 25: 26\n",
      "\n",
      "=======DATASET 26=======\n",
      "\n",
      "length of dataset 26: 26\n",
      "\n",
      "=======DATASET 27=======\n",
      "\n",
      "length of dataset 27: 26\n",
      "\n",
      "=======DATASET 28=======\n",
      "\n",
      "length of dataset 28: 26\n",
      "\n",
      "=======DATASET 29=======\n",
      "\n",
      "length of dataset 29: 26\n",
      "\n",
      "=======DATASET 30=======\n",
      "\n",
      "length of dataset 30: 25\n",
      "['a.png', 'b.png', 'c.png', 'd.png', 'e.png', 'f.png', 'g.png', 'h.png', 'i.png', 'k.png', 'l.png', 'm.png', 'n.png', 'o.png', 'p.png', 'q.png', 'r.png', 's.png', 't.png', 'u.png', 'v.png', 'w.png', 'x.png', 'y.png', 'z.png']\n"
     ]
    }
   ],
   "source": [
    "# make sure each directory has 26 images\n",
    "\n",
    "for n in range(1, 31):\n",
    "    print(f'\\n=======DATASET {n}=======\\n')\n",
    "    path = f\"./og_data/{n}\"\n",
    "    filenames = [f for f in os.listdir(path) \n",
    "                 if os.path.isfile(os.path.join(path, f)) \n",
    "                 and 'checkpoint' not in f]\n",
    "    \n",
    "    print(f'length of dataset {n}: {len(filenames)}')\n",
    "    if len(filenames) != 26: \n",
    "        print(sorted(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data \n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self sufficient and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
